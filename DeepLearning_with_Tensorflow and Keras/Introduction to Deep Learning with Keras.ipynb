{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hello = tf.constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from keras.layers import BatchNormalization\n",
    "# model.add(BatchNormalization()) - add to the output of each layers before the final layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE\n",
    "\n",
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.read_csv('')\n",
    "predictors = dff.drop(['target'], axis = 1).as_matrix()\n",
    "target = to_categorical(dff.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifying the model\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(100, activation = 'relu', input_shape = (n_cols,)))\n",
    "model.add(Dense(100, activation = 'relu'))\n",
    "model.add(Dense(2, activation = 'sigmoid')) ## for classification models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compiling and fitting the model\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy']) ## classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Saving, Reloading and Using your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_file.h5')\n",
    "my_model = load_model('model_file.h5')\n",
    "predictions = my_model.predict('')\n",
    "probability_true = predictions[:,1]\n",
    "\n",
    "\n",
    "my_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Using multiple learning rates simultaneously\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "def get_new_model(input_shape = input_shape):\n",
    "    model.Sequential()\n",
    "    model.add(Dense(100, activation = 'relu', input_shape = input_shape))\n",
    "    model.add(Dense(100, activation = 'relu'))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "    return(model)\n",
    "\n",
    "\n",
    "lr_to_test = [0.0001, 0.01, 1]\n",
    "\n",
    "## loop over learning rates\n",
    "## Optimization\n",
    "\n",
    "for lr in lr_to_test:\n",
    "    model = get_new_model()\n",
    "    my_optimizer = SGD(lr = lr)\n",
    "    model.compile(optimizer = my_optimizer, loss = 'categorical_crossentropy')\n",
    "    model.fit(predictors, target)\n",
    "    \n",
    "    # Evaluate your model   \n",
    "    print(\"Final loss value:\",model.evaluate(____, ____))[1]   ## accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## opt = Adam(lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Early Stopping\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping_monitor = EarlyStopping(patience=2)\n",
    "model.fit(predictors, target, validation_split = 0.3, epochs = 20, callbacks = [early_stopping_monitor])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Import to_categorical from keras utils module\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "# Use to_categorical on your labels\n",
    "competitors = coordinates(darts.competitor)\n",
    "\n",
    "# Now print the one-hot encoded labels\n",
    "print('One-hot encoded competitors: \\n',competitors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(coord_train, competitors_train, epochs=200)\n",
    "\n",
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test, competitors_test)[1]\n",
    "\n",
    "# Print accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on coords_small_test\n",
    "preds = model.predict(coords_small_test)\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:45} | {}\".format('Raw Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds):\n",
    "  print(\"{} | {}\".format(pred,competitors_small_test[i]))\n",
    "\n",
    "# Extract the position of highest probability from each pred vector\n",
    "preds_chosen = [np.argmax(pred) for pred in preds]\n",
    "\n",
    "# Print preds vs true values\n",
    "print(\"{:10} | {}\".format('Rounded Model Predictions','True labels'))\n",
    "for i,pred in enumerate(preds_chosen):\n",
    "  print(\"{:25} | {}\".format(pred,competitors_small_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save its history\n",
    "h_callback = model.fit(X_train, y_train, epochs = 50,\n",
    "                    validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(h_callback.history['loss'], h_callback.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(h_callback.history['acc'], h_callback.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor= 'val_acc', \n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test, y_test),\n",
    "           callbacks= [monitor_val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor ='val_acc', patience = 3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only = True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "h_callback = model.fit(X_train, y_train,\n",
    "                    epochs = 1000000000000,\n",
    "                    callbacks = [monitor_val_acc,modelCheckpoint],\n",
    "                    validation_data = (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = model.get_weights()\n",
    "train_accs = []\n",
    "test_accs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for size in training_sizes:\n",
    "  \t# Get a fraction of training data (we only care about the training data)\n",
    "    X_train_frac, y_train_frac = X_train[:size], y_train[:size]\n",
    "\n",
    "    # Reset the model to the initial weights and train it on the new training data fraction\n",
    "    model.set_weights(initial_weights)\n",
    "    model.fit(X_train_frac, y_train_frac, epochs = 50, callbacks = [early_stop])\n",
    "\n",
    "    # Evaluate and store both: the training data fraction and the complete test set results\n",
    "    train_accs.append(model.evaluate(X_train_frac, y_train_frac, verbose = 0)[1])\n",
    "    test_accs.append(model.evaluate(X_test, y_test, verbose = 0)[1])\n",
    "    \n",
    "# Plot train vs test accuracies\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Comparing Activation Functions\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "def get_model(act_function):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, activation = act_function, input_shape = input_shape))\n",
    "    model.add(Dense(100, activation = act_function))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "    return model\n",
    "\n",
    "## sigmoid used for binary classification\n",
    "## softmax used for multi-class classification\n",
    "\n",
    "## Activations to try out\n",
    "activation = ['relu','sigmoid','tanh']\n",
    "\n",
    "activation_results = {}\n",
    "\n",
    "for funct in activation:\n",
    "    model = get_model(act_function = funct)\n",
    "    history = model.fit(X_train, y_train, validation_data = (X_test, y_test), epochs = 100, verbose = 0)\n",
    "    activation_results[funct] = history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "## Extract val_loss history of each activation function\n",
    "val_loss_per_funct = {k:v.history['val_loss'] for k,v in activation_results.items()}\n",
    "\n",
    "## Turn the dictionary into dataframe\n",
    "val_loss_curves = pd.DataFrame(val_loss_per_funct)\n",
    "\n",
    "## plot the curves\n",
    "val_loss_curves.plot(title = 'Loss Per Activation Function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameter Tuning\n",
    "Neural Network Hyperparameters\n",
    "1. Number of Layers\n",
    "2. Number of neurons per layer\n",
    "3. Layer order\n",
    "4. Layer activations\n",
    "5. Batch sizes\n",
    "6. Learning_rates\n",
    "7. Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Hyperparameter Tuning in Keras\n",
    "\n",
    "def create_model(opt, act):\n",
    "    model.Sequential()\n",
    "    model.add(Dense(100, activation = act, input_shape = input_shape))\n",
    "    model.add(Dense(100, activation = act))\n",
    "    model.add(Dense(2, activation = 'sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer = opt, loss = 'binary_crossentropy')\n",
    "    return(model)\n",
    "\n",
    "## import sklearn wrapper from keras\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "## Create a model as a sklearn estimator\n",
    "model = KerasClassifier(build_fn = create_model(opt = 'adam', act = 'relu'), epochs = 6, batch_size = 10)\n",
    "\n",
    "## import cross_val_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## check how your model performs with 5-fold cross validation\n",
    "kfold = cross_val_score(model, X,y, cv=5)\n",
    "\n",
    "## You can check the mean and std using kfold.mean(), kfold.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tips for neural networks hyperparameter tuning\n",
    "\n",
    "1. Random Search is better tha grid search\n",
    "2. Dont use too many epochs\n",
    "3. Use a smaller sample of your dataset\n",
    "4. Play with batch_sizes, optimizers, activations and learning_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Random Search on Keras Models\n",
    "\n",
    "# Define a series of parameters\n",
    "params = dict(optimizer = ['sgd', 'adam'], epochs = 3, 'learning_rate':[0.1, 0.01, 0.001], batch_size = [5,10,20], activation = ['relu', 'tanh'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a RandomizedSearchCV object and fit to the data\n",
    "random_search = RandomizedSearchCV(model, params_dist = params, cv =3)\n",
    "random_search_results = random_search.fit(X,y)\n",
    "\n",
    "## Print results\n",
    "print(\"Best: %f using %s\".format(random_search_results.best_score_, random_search_results.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tuning other hyperparameters\n",
    "\n",
    "def create_model(nl=1, nn=256):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(16, input_shape = (2,), activation='relu'))\n",
    "    ## Add as many hidden layers as specified in nl\n",
    "    for i in range(nl):\n",
    "        model.add(Dense(nn, activation = 'relu'))\n",
    "        \n",
    "params = dict(nl = [1,2,9], nn = [128,256,1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Accessing the first layer of a keras model\n",
    "first_model = model.layers[0]\n",
    "print(first_layer.input)\n",
    "print(first_layer.output)\n",
    "print(first_layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Conv2D layer and flatten from keras.layers\n",
    "from keras.layers import Dense, Conv2D, Flatten\n",
    "\n",
    "## Instantiate your model\n",
    "model = Sequential()\n",
    "\n",
    "## Add a convolutional layer with 32 filters of size 3x3\n",
    "model.add(Conv2D(filters = 32, kernel_size = 3, input_shape = (28,28,1), activation = 'relu'))\n",
    "\n",
    "## Add another convolutional layer\n",
    "model.add(Conv2D(8, kernel_size = 3, activation = 'relu'))\n",
    "\n",
    "## Flatten the output of the previous layer\n",
    "model.add(Flatten())\n",
    "\n",
    "## End the multiclass model with 3 outputs and softmax\n",
    "model.add(Dense(3, activation = 'softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRANSFER LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import image and preprocess_input\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "\n",
    "# Load the image with the right target size for your model\n",
    "img = image.load_img(img_path, target_size = (224, 224))\n",
    "\n",
    "# Turn it into an array\n",
    "img_array = image.img_to_array(img)\n",
    "\n",
    "# Expand the dimensions of the image, this is so that it fits the expected model input format\n",
    "img_expanded = np.expand_dims(img_array, axis = 0)\n",
    "\n",
    "# Pre-process the img in the same way original images were\n",
    "img_ready = preprocess_input(img_expanded)\n",
    "\n",
    "# Instantiate a ResNet50 model with 'imagenet' weights\n",
    "model = ResNet50(weights='imagenet')\n",
    "\n",
    "# Predict with ResNet50 on your already processed img\n",
    "preds = model.predict(img_ready)\n",
    "\n",
    "# Decode the first 3 predictions\n",
    "print('Predicted:', decode_predictions(preds, top=3)[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
